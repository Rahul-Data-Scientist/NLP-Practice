{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f3876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ec50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6e0a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10391be1",
   "metadata": {},
   "source": [
    "# Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1decda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002fc7a",
   "metadata": {},
   "source": [
    "# Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999498d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4ec7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Rahul! Welcome to our website. This is a sample paragraph with HTML tags.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '<p>Hello <b>Rahul</b>! Welcome to <a href=\"https://example.com\">our website</a>.</p> <div>This is a <span style=\"color:red;\">sample</span> paragraph with <i>HTML</i> tags.</div>'\n",
    "\n",
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7f517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ba3de",
   "metadata": {},
   "source": [
    "# Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca869f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdbb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add0e6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For notebook click  to search check '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de04b8c",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fef904cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "922fd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b06c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe09fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'string. With. Punctuation?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6388130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string With Punctuation'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa9ef23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process one input: 6.160000339150429e-05 sec\n",
      "So, time taken to process 50K inputs: 3.0800001695752144 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "remove_punc(text)\n",
    "time1 = time.perf_counter() - start\n",
    "\n",
    "print(\"Time taken to process one input:\", time1, \"sec\")\n",
    "print(\"So, time taken to process 50K inputs:\", time1 * 50000, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fd7e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster method\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "639f16d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process one input: 4.499999340623617e-05 sec\n",
      "So, time taken to process 50K inputs: 2.2499996703118086 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "remove_punc1(text)\n",
    "time1 = time.perf_counter() - start\n",
    "\n",
    "print(\"Time taken to process one input:\", time1, \"sec\")\n",
    "print(\"So, time taken to process 50K inputs:\", time1 * 50000, \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489e4e2",
   "metadata": {},
   "source": [
    "# Chat Word treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4abe43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"ADIH\": \"Another Day In Hell\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"BAE\": \"Before Anyone Else\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRUH\": \"Bro\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BSAAW\": \"Big Smile And A Wink\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"BWL\": \"Bursting With Laughter\",\n",
    "    \"CSL\": \"Can‚Äôt Stop Laughing\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"DM\": \"Direct Message\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FIMH\": \"Forever In My Heart\",\n",
    "    \"FOMO\": \"Fear Of Missing Out\",\n",
    "    \"FR\": \"For Real\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYP\": \"For You Page\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GOAT\": \"Greatest Of All Time\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"HBD\": \"Happy Birthday\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek You\",\n",
    "    \"IDC\": \"I Don‚Äôt Care\",\n",
    "    \"IDK\": \"I Don't Know\",\n",
    "    \"IFYP\": \"I Feel Your Pain\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"ILY\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMU\": \"I Miss You\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"IYKYK\": \"If You Know, You Know\",\n",
    "    \"JK\": \"Just Kidding\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"L\": \"Loss\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMK\": \"Let Me Know\",\n",
    "    \"LMAO\": \"Laughing My A** Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"MFW\": \"My Face When\",\n",
    "    \"MID\": \"Mediocre\",\n",
    "    \"MRW\": \"My Reaction When\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"NVM\": \"Never Mind\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"NPC\": \"Non-Player Character\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"OP\": \"Overpowered\",\n",
    "    \"PITA\": \"Pain In The A**\",\n",
    "    \"POV\": \"Point Of View\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A** Off\",\n",
    "    \"RN\": \"Right Now\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your Sex And Age\",\n",
    "    \"SUS\": \"Suspicious\",\n",
    "    \"TBH\": \"To Be Honest\",\n",
    "    \"TFW\": \"That Feeling When\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TIME\": \"Tears In My Eyes\",\n",
    "    \"TLDR\": \"Too Long, Didn‚Äôt Read\",\n",
    "    \"TNTL\": \"Trying Not To Laugh\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"W\": \"Win\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F**k\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"WYD\": \"What You Doing?\",\n",
    "    \"WYWH\": \"Wish You Were Here\",\n",
    "    \"ZZZ\": \"Sleeping, Bored, Tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51dcf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95f0c22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For Your Information New Delhi is the capital of India'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('FYI New Delhi is the capital of India')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae6a01",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b01f245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ef57d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "textblb = TextBlob(incorrect_text)\n",
    "textblb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed33b243",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "448d3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9eaba20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d16bcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f049169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This simple sentence written purpose demonstrating stopwords clear way.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"This is a simple sentence that is written for the purpose of demonstrating stopwords in a clear way.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17120b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af56fba",
   "metadata": {},
   "source": [
    "# Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e7bb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Emoji\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f99d0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie. It was üòçüòò\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed4eb58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lmao '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Lmao üòÇüòÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dde1e523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :smiling_face_with_heart-eyes::face_blowing_a_kiss:\n"
     ]
    }
   ],
   "source": [
    "# Replace Emoji\n",
    "\n",
    "import emoji\n",
    "print(emoji.demojize(\"Loved the movie. It was üòçüòò\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea3caa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Python is üî•'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19f2cf",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec03f76",
   "metadata": {},
   "source": [
    "### 1. Using the split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "665df43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1 = \"i am going to delhi\"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "045bc1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to Delhi', ' I am gonna stay there for 3 days']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = \"I am going to Delhi. I am gonna stay there for 3 days\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3270d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Delhi!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problems with split function\n",
    "sent3 = \"I am going to Delhi!\"\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7152f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See. Delhi! is appearning together instead of 'Delhi' and '!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "612c6129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do you think I should go? I have 3 days leave']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = \"Where do you think I should go? I have 3 days leave\"\n",
    "sent4.split(\".\")\n",
    "# We can see in the output that split function method is no tdoing the desired sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb27023",
   "metadata": {},
   "source": [
    "### 2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0afbfe7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3 = 'I am going to delhi!'\n",
    "tokens = re.findall(\"[\\w]+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1570a90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14dff256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But still we have to design regex based on our data. It is more convenient to use standard libraries like NLTK and Spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5848989",
   "metadata": {},
   "source": [
    "### 3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2418fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94cdfc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Delhi', '!']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"I am going to Delhi!\"\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3cf50d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5f88c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79261d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see above in the second output, NLTK is also not perfect as it is splitting the EmailID. It can make mistakes. Spacy generally performs better. But NLTK is aso very good as we can see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf5962",
   "metadata": {},
   "source": [
    "### 4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1b6f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp(sent5)\n",
    "\n",
    "# spacy import is resulting in some error because of some conflict with PyTorch. Need some fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d68261",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0cdb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b403c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join(ps.stem(word) for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfd6eba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b02dfa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studi studi studi'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"study studies studying\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae1697b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See. That's the problem with stemming. It doesn't always produce a real word. \n",
    "# That's why Lemmatization is used which produces real words but takes more time than stemming.\n",
    "# So, if we need to present the reduced words to the user, then we must use lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a8710",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0a061b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a208266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos = 'v')))  # by pos parameter, we are specifying part of speech ki aapko kis form mein word chahiye. pos = 'v' means hume words verb form mein chahiye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395f335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
