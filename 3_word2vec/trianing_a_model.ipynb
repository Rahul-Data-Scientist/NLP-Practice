{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_icQxOfAPaT"
      },
      "outputs": [],
      "source": [
        "# Here, we will train a model on game of thrones story data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "51UIdqJtAbmP",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012630b4-5a8a-4300-8440-cd4a1985d85a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "CCaI90DdCvcd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "bKN1fmTGFL5I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmQYU_ltE8be",
        "outputId": "8a93bf08-2a85-488d-d8ed-533b244e0463"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "story = []\n",
        "\n",
        "for filename in [\"/content/001ssb.txt\", \"/content/002ssb.txt\", \"/content/003ssb.txt\", \"/content/004ssb.txt\", \"/content/005ssb.txt\"]:\n",
        "  with open(filename, \"r\", encoding = \"utf-8\", errors = \"ignore\") as f:\n",
        "    corpus = f.read()\n",
        "    raw_sent = sent_tokenize(corpus)\n",
        "    for sent in raw_sent:\n",
        "      story.append(simple_preprocess(sent))"
      ],
      "metadata": {
        "id": "p7YYaOsZC2Iq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(story[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dERZZTkPEYy-",
        "outputId": "2ffa0bef-57e8-4b9f-f4ad-9b2ed1890ff3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['game', 'of', 'thrones', 'book', 'one', 'of', 'song', 'of', 'ice', 'and', 'fire', 'by', 'george', 'martin', 'prologue', 'we', 'should', 'start', 'back', 'gared', 'urged', 'as', 'the', 'woods', 'began', 'to', 'grow', 'dark', 'around', 'them'], ['the', 'wildlings', 'are', 'dead']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(\n",
        "    window = 5,   # window = 5 means hum middle word ke left ke 5 words and right ke 5 words ko context maan rahe hain.\n",
        "    vector_size = 100, # size of embeddings\n",
        "    min_count = 5 # consider those sentences which have greater than or equal to 5 words\n",
        ")"
      ],
      "metadata": {
        "id": "9VdjE7NcFnwt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(story)"
      ],
      "metadata": {
        "id": "WwHAmU0THDOj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(\n",
        "    story,\n",
        "    total_examples = model.corpus_count,\n",
        "    epochs = model.epochs\n",
        ")\n",
        "\n",
        "# model.train() trains the Word2Vec model on our tokenized sentences\n",
        "#\n",
        "# story → training corpus (list of tokenized sentences)\n",
        "# total_examples → total number of sentences used for training (same as used in build_vocab)\n",
        "# epochs → number of full passes over the entire dataset. THe default value of model.epochs = 5. But if we wanted 10, then before calling model.train, explicitly do model.epochs = 10\n",
        "#\n",
        "# In simple words:\n",
        "# The model reads all sentences `epochs` times and learns word embeddings from context windows."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPSzyGT6HGU9",
        "outputId": "157e133a-163f-4f11-9c82-3f2053f3182d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6485021, 8625265)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As we can see, model.trained returned (trained_words, total_words)\n",
        "# It means 6485021 words were actually trained out of 8625265 total words available in the corpus"
      ],
      "metadata": {
        "id": "y5wepS--IXqr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"daenerys\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TXTM0xGI35A",
        "outputId": "632321b2-f493-42c1-a269-e595c59b458e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('targaryen', 0.8108153939247131),\n",
              " ('rhaegar', 0.7588443160057068),\n",
              " ('stormborn', 0.7552147507667542),\n",
              " ('myrcella', 0.7427701354026794),\n",
              " ('martell', 0.7214646339416504),\n",
              " ('doran', 0.7076268792152405),\n",
              " ('aegon', 0.7033505439758301),\n",
              " ('unburnt', 0.6930316686630249),\n",
              " ('princess', 0.6911218762397766),\n",
              " ('elia', 0.683988630771637)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.doesnt_match(['jon', 'rikon', 'robb', 'arya', 'sansa', 'bran'])\n",
        "# wv stands for Word Vectors\n",
        "# Output: 'jon' -> as jon ke alaawa baaki sab bhai behan hain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "EsmwcscRJBUV",
        "outputId": "82e17fad-9df7-4ac9-9dcb-9ee7ba97cb41"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:vectors for words {'rikon'} are not present in the model, ignoring these words\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jon'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.doesnt_match(['cersei', 'jaime', 'bronn', 'tyrion'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yBs1p1PfJQGL",
        "outputId": "3a71d777-eb64-4ca5-e8fe-03380de3fd91"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bronn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv[\"jon\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncx2rnQGJTz9",
        "outputId": "013bd0fd-669f-47c2-d4c7-83c8a68b623a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.1417359 , -0.76701295, -0.04729752, -1.9475315 ,  0.6180081 ,\n",
              "        1.2416173 ,  0.35969636,  0.03677973, -1.584901  , -0.761219  ,\n",
              "        0.22449465, -0.8442932 , -1.5748904 ,  1.0577421 , -1.8903779 ,\n",
              "       -0.11233115,  0.5369906 ,  1.8036904 , -1.0232834 , -1.7875987 ,\n",
              "        2.2877347 , -0.44566616,  0.3230564 ,  1.4278219 , -1.3681276 ,\n",
              "        1.2708513 ,  0.6455826 ,  0.56810904,  0.04345945,  0.10140568,\n",
              "        0.09782472,  0.23890033,  0.35742068,  0.28402495,  0.39344025,\n",
              "        0.8905451 ,  0.33072942,  0.26133546,  0.72246754,  0.71952194,\n",
              "       -1.0771602 , -1.825463  , -0.41441748,  1.852197  ,  0.8623074 ,\n",
              "       -0.20993707,  0.12177436, -0.15658516, -0.27558976, -0.5777635 ,\n",
              "        1.2757686 , -0.6370818 ,  2.0504487 , -0.6105739 , -0.39468333,\n",
              "        1.4488578 , -0.8815424 , -2.2243495 , -0.11393473,  0.09227539,\n",
              "        0.6915091 ,  0.26477367, -0.3430763 , -0.4684272 ,  0.09704905,\n",
              "       -0.5732741 ,  1.3105139 ,  1.6315547 ,  0.20728004, -0.16931148,\n",
              "        2.0626423 , -0.8456022 , -1.3782309 , -0.09490544,  0.15070693,\n",
              "        2.188459  , -0.51147544,  2.7164786 ,  1.2174547 , -0.82609904,\n",
              "        0.01098707, -0.5779353 , -1.8831807 , -0.7392975 , -1.7244159 ,\n",
              "        1.7492343 ,  0.19874568, -0.5490316 , -0.31823197, -0.4065885 ,\n",
              "        0.40020806,  1.4293205 ,  0.4441006 ,  1.4735149 ,  1.8198206 ,\n",
              "       -0.93690425, -0.22544771, -1.8567983 ,  0.1618069 ,  1.8224272 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similarity(\"tywin\", \"sansa\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie0GxvrhJjxC",
        "outputId": "8f0ecf80-ea53-4b07-de53-c51ec2b9c9ef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.33564815)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.wv.get_normed_vectors()\n",
        "# # through above code, word vectors in the vocabulary are returned in normalized form."
      ],
      "metadata": {
        "id": "XcQOT6EGJtNT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.get_normed_vectors().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYXj42vwJ26a",
        "outputId": "c7a9656b-64c7-4a2f-9593-28e39cf6eb45"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11975, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Understanding Word2Vec tokens vs vocabulary\n",
        "#\n",
        "# model.train() returns (trained_tokens, total_tokens)\n",
        "# - trained_tokens = total word occurrences used for training (after filtering by min_count = 5)\n",
        "# - total_tokens = all word occurrences in the corpus\n",
        "#\n",
        "# model.wv.get_normed_vectors().shape returns (vocab_size, embedding_dim)\n",
        "# - vocab_size = number of unique words that have embeddings\n",
        "# - embedding_dim = dimensionality of each word vector\n",
        "#\n",
        "# ✅ Key point:\n",
        "# Many word occurrences (tokens) contribute to training each word vector,\n",
        "# but only one vector is stored per unique word."
      ],
      "metadata": {
        "id": "8tWFAvK2KQXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = model.wv.index_to_key\n",
        "# y contains a list of all unique words in your vocabulary, ordered by decreasing frequency.\n",
        "# y[0] → the most frequent word in your corpus (after min_count filtering)\n",
        "# y[1] → the second most frequent word"
      ],
      "metadata": {
        "id": "uxjInxNDLjhw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4N5e3TTLm6g",
        "outputId": "e383e59a-c5a5-401f-d312-d1cc2be9748d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'and', 'to', 'of', 'he']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output mein saare stopwords hain kyunki humne stopwords bina hataaye yeh kaam kiya hai."
      ],
      "metadata": {
        "id": "OJTEEVAxLnRx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}